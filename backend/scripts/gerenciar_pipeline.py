import os
import shutil
import json
import subprocess
import argparse
from pathlib import Path
from urllib.parse import urlparse, parse_qs

import whisper
import fitz  # PyMuPDF --- NOVO ---
from dotenv import load_dotenv

from langchain.docstore.document import Document
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException

# --- 1. CONFIGURA√á√ïES E CONSTANTES GLOBAIS ---
BACKEND_DIR = Path(__file__).parent.parent.resolve()
DATA_DIR = BACKEND_DIR / "data"
VIDEOS_DIR = BACKEND_DIR / "videos"
CHROMA_PATH = str(BACKEND_DIR / "chroma_db_local")
LINKS_FILE = BACKEND_DIR / "youtube_links.txt"
TEMP_DIR = BACKEND_DIR / "temp_audio"
REFINER_PROMPT_JSON_TEMPLATE = """
Voc√™ √© um sistema especialista em ETL (Extra√ß√£o, Transforma√ß√£o e Carga) de conhecimento. Sua fun√ß√£o √© receber um trecho de uma transcri√ß√£o de v√≠deo-aula ou um texto de um documento e transform√°-lo em um ou mais "chunks" de conhecimento em formato JSON. Cada chunk deve ser at√¥mico, coeso e focado em um √∫nico t√≥pico ou subt√≥pico. O objetivo √© criar uma base de dados vetorial otimizada para buscas de similaridade (RAG).

**Instru√ß√µes Detalhadas:**

1.  **Analise o Texto:** Leia o conte√∫do fornecido e identifique os principais conceitos, explica√ß√µes, exemplos ou instru√ß√µes.
2.  **Segmente em Chunks:** Divida o texto em segmentos l√≥gicos. Um √∫nico bloco de texto pode se tornar um ou v√°rios chunks, dependendo da densidade da informa√ß√£o.
3.  **Gere T√≠tulos Curtos e Descritivos:** Para cada chunk, crie um `title` que resuma o conte√∫do de forma clara e concisa (m√°ximo de 10 palavras).
4.  **Formate o Conte√∫do:** O campo `content` deve ser o texto do chunk, otimizado para clareza.
5.  **Defina o M√≥dulo:** No campo `module`, categorize o chunk em uma das seguintes √°reas de conhecimento: "Vendas", "Produto", "Marketing", "Negocia√ß√£o", "Geral".
6.  **Atribua Tags:** No campo `tags`, forne√ßa uma lista de 3 a 5 palavras-chave relevantes.
7.  **Estrutura de Sa√≠da:** Sua sa√≠da DEVE ser uma lista de objetos JSON.

**Exemplo de Sa√≠da JSON Esperada (DEVE ser uma lista):**
[
  {{
    "title": "Qualifica√ß√£o de Leads com BANT",
    "content": "A qualifica√ß√£o de leads √© um processo crucial em vendas. Uma metodologia eficaz √© o BANT, que avalia quatro crit√©rios principais: Budget (Or√ßamento), Authority (Autoridade), Need (Necessidade) e Timeline (Prazo).",
    "module": "Vendas",
    "tags": ["BANT", "qualifica√ß√£o", "lead", "budget", "vendas"]
  }},
  {{
    "title": "An√°lise de Concorr√™ncia em Vendas",
    "content": "Para um posicionamento estrat√©gico eficaz, √© fundamental realizar o mapeamento da concorr√™ncia, identificando tanto os concorrentes diretos quanto os indiretos.",
    "module": "Negocia√ß√£o",
    "tags": ["concorr√™ncia", "an√°lise de mercado", "posicionamento", "estrat√©gia"]
  }}
]

**Conte√∫do para Processar:**
{transcription_text}
"""
REFINER_PROMPT_JSON = ChatPromptTemplate.from_template(REFINER_PROMPT_JSON_TEMPLATE)


# --- 2. FUN√á√ïES DO PIPELINE ---

def transcribe_youtube_video(url: str, model) -> Path | None:
    # ... (c√≥digo inalterado)
    try:
        video_id = parse_qs(urlparse(url).query)['v'][0]
        json_path = DATA_DIR / f"youtube_{video_id}.json"
        if json_path.exists():
            print(f"  -> ‚è≠Ô∏è  Transcri√ß√£o (YouTube) j√° existe para '{url}'. Pulando.")
            return json_path
        TEMP_DIR.mkdir(exist_ok=True)
        audio_filepath = TEMP_DIR / f"{video_id}.mp3"
        print(f"  -> üé§ Baixando e transcrevendo √°udio (YouTube)...")
        command = ['yt-dlp', '-x', '--audio-format', 'mp3', '-o', str(audio_filepath), url]
        subprocess.run(command, check=True, timeout=300)
        if not audio_filepath.exists(): raise FileNotFoundError("Download do √°udio falhou.")
        result = model.transcribe(str(audio_filepath), verbose=False, language="pt")
        output_data = [{"text": seg["text"].strip(), "source_name": url} for seg in result["segments"]]
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        print(f"  -> ‚úÖ Transcri√ß√£o (YouTube) salva em '{json_path.name}'")
        return json_path
    except Exception as e:
        print(f"  -> ‚ùå ERRO ao transcrever a URL '{url}': {e}")
        return None
    finally:
        if 'audio_filepath' in locals() and audio_filepath.exists(): os.remove(audio_filepath)


def transcribe_local_video(video_path: Path, model) -> Path | None:
    # ... (c√≥digo inalterado)
    try:
        video_id = video_path.stem
        json_path = DATA_DIR / f"local_{video_id}.json"
        if json_path.exists():
            print(f"  -> ‚è≠Ô∏è  Transcri√ß√£o (Local) j√° existe para '{video_path.name}'. Pulando.")
            return json_path
        print(f"  -> üé§ Transcrevendo v√≠deo local: '{video_path.name}'...")
        result = model.transcribe(str(video_path), verbose=False, language="pt")
        output_data = [{"text": seg["text"].strip(), "source_name": video_path.name} for seg in result["segments"]]
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        print(f"  -> ‚úÖ Transcri√ß√£o (Local) salva em '{json_path.name}'")
        return json_path
    except Exception as e:
        print(f"  -> ‚ùå ERRO ao transcrever o v√≠deo local '{video_path.name}': {e}")
        return None


# --- NOVO ---: Fun√ß√µes para processar arquivos TXT e PDF
def process_text_file(file_path: Path) -> Path | None:
    """L√™ um arquivo .txt e o converte para o formato JSON intermedi√°rio."""
    try:
        file_id = file_path.stem
        json_path = DATA_DIR / f"doc_{file_id}.json"
        if json_path.exists():
            print(f"  -> ‚è≠Ô∏è  Processamento de texto j√° existe para '{file_path.name}'. Pulando.")
            return json_path

        print(f"  -> üìÑ Processando arquivo de texto: '{file_path.name}'...")
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Estrutura como um √∫nico "segmento" para compatibilidade com a fun√ß√£o de refinamento
        output_data = [{"text": content.strip(), "source_name": file_path.name}]

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        print(f"  -> ‚úÖ Processamento de texto salvo em '{json_path.name}'")
        return json_path
    except Exception as e:
        print(f"  -> ‚ùå ERRO ao processar o arquivo de texto '{file_path.name}': {e}")
        return None


def process_pdf_file(file_path: Path) -> Path | None:
    """L√™ um arquivo .pdf, extrai o texto e o converte para o formato JSON intermedi√°rio."""
    try:
        file_id = file_path.stem
        json_path = DATA_DIR / f"doc_{file_id}.json"
        if json_path.exists():
            print(f"  -> ‚è≠Ô∏è  Processamento de PDF j√° existe para '{file_path.name}'. Pulando.")
            return json_path

        print(f"  -> üìÑ Processando arquivo PDF: '{file_path.name}'...")
        doc = fitz.open(file_path)
        content = ""
        for page in doc:
            content += page.get_text()
        doc.close()

        output_data = [{"text": content.strip(), "source_name": file_path.name}]

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        print(f"  -> ‚úÖ Processamento de PDF salvo em '{json_path.name}'")
        return json_path
    except Exception as e:
        print(f"  -> ‚ùå ERRO ao processar o arquivo PDF '{file_path.name}': {e}")
        return None


def refine_single_json_file(json_filepath: Path, chain, source_type: str):
    # ... (c√≥digo inalterado)
    if not json_filepath or not json_filepath.exists(): return
    output_filepath = DATA_DIR / f"refinado_{json_filepath.stem}.jsonl"
    if output_filepath.exists():
        print(f"  -> ‚è≠Ô∏è  Arquivo refinado '{output_filepath.name}' j√° existe. Pulando refinamento.")
        return
    try:
        print(f"  -> üß† Refinando {json_filepath.name} com IA...")
        with open(json_filepath, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        if not segments: return
        source_name = segments[0].get("source_name", "Fonte Desconhecida")
        total_chunks_created = 0

        # --- NOVO ---: L√≥gica adaptada para documentos e v√≠deos
        # Para documentos, processamos o texto inteiro. Para v√≠deos, em blocos.
        if "start" in segments[0]:  # Heur√≠stica para detectar se √© de v√≠deo
            SEGMENTS_PER_BLOCK = 15
        else:  # Se for documento, processar tudo de uma vez
            SEGMENTS_PER_BLOCK = len(segments)

        with open(output_filepath, 'w', encoding='utf-8') as f:
            for i in range(0, len(segments), SEGMENTS_PER_BLOCK):
                block_segments = segments[i:i + SEGMENTS_PER_BLOCK]
                block_text = " ".join([seg['text'] for seg in block_segments])

                try:
                    refined_data_list = chain.invoke({"transcription_text": block_text})
                except (OutputParserException, json.JSONDecodeError) as e:
                    print(f"  -> ‚ö†Ô∏è AVISO: Falha ao analisar a resposta da IA para um bloco. Pulando. Erro: {e}")
                    continue

                if not isinstance(refined_data_list, list) or not refined_data_list: continue
                for idx, chunk_data in enumerate(refined_data_list):
                    chunk_id = f"{json_filepath.stem}_{i}_{idx}"
                    metadata = {
                        "source_type": source_type, "source_name": source_name,
                        "module": chunk_data.get("module", "Geral"),
                        "tags": chunk_data.get("tags", [])
                    }
                    # Adiciona metadados de tempo apenas se existirem
                    if "start" in block_segments[0]:
                        metadata["start_time"] = round(block_segments[0].get('start', 0))
                        metadata["end_time"] = round(block_segments[-1].get('end', 0))

                    final_chunk = {
                        "chunk_id": chunk_id, "source_document_id": json_filepath.stem,
                        "title": chunk_data.get("title", "Sem T√≠tulo"),
                        "content": chunk_data.get("content", ""),
                        "metadata": metadata
                    }
                    f.write(json.dumps(final_chunk, ensure_ascii=False) + '\n')
                    total_chunks_created += 1
        print(f"  -> ‚úÖ {total_chunks_created} chunks salvos em: '{output_filepath.name}'")
    except Exception as e:
        print(f"  -> ‚ùå ERRO GERAL ao refinar o arquivo '{json_filepath.name}': {e}")


def create_database_from_all_jsonl():
    # ... (c√≥digo inalterado)
    print("\n--- [FINAL] CRIANDO O BANCO DE DADOS VETORIAL ---")
    all_chunks = []
    jsonl_files = list(DATA_DIR.glob("refinado_*.jsonl"))
    if not jsonl_files:
        print("AVISO: Nenhum arquivo .jsonl encontrado para criar o banco de dados.")
        return
    for file_path in jsonl_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if "tags" in data["metadata"] and isinstance(data["metadata"]["tags"], list):
                        data["metadata"]["tags"] = ", ".join(data["metadata"]["tags"])
                    doc = Document(page_content=data["content"],
                                   metadata={**data["metadata"], "chunk_id": data["chunk_id"], "title": data["title"]})
                    all_chunks.append(doc)
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"  -> ‚ö†Ô∏è AVISO: Pulando linha mal formada no arquivo '{file_path.name}'. Erro: {e}")
    if not all_chunks:
        print("\nAVISO: Nenhum chunk v√°lido foi extra√≠do para adicionar ao banco de dados.")
        return
    print(f"INFO: Total de {len(all_chunks)} chunks para adicionar ao DB.")
    api_key = os.environ.get("GEMINI_API_KEY")
    embeddings_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
    if os.path.exists(CHROMA_PATH): shutil.rmtree(CHROMA_PATH)
    print("  -> Inicializando novo banco de dados ChromaDB...")
    db = Chroma.from_documents(
        documents=all_chunks,
        embedding=embeddings_model,
        persist_directory=CHROMA_PATH
    )
    print(f"  -> Adicionados {len(all_chunks)} chunks em uma √∫nica opera√ß√£o.")
    db.persist()
    print("‚úÖ Banco de Dados criado/atualizado com sucesso!")


# --- 3. ORQUESTRADOR PRINCIPAL ---
def main():
    parser = argparse.ArgumentParser(description="Pipeline de gest√£o da base de conhecimento do RAG.")
    parser.add_argument('--full-rebuild', action='store_true',
                        help="For√ßa a limpeza dos .jsonl e a recria√ß√£o do conhecimento.")
    args = parser.parse_args()
    print("--- INICIANDO PIPELINE DE GEST√ÉO DA BASE DE CONHECIMENTO ---")
    load_dotenv()
    if args.full_rebuild:
        print("\n--- MODO RECONSTRU√á√ÉO COMPLETA ATIVADO ---")
        print("INFO: Limpando arquivos .jsonl e .json intermedi√°rios...")
        deleted_files_count = 0
        for filename in os.listdir(DATA_DIR):
            if filename.endswith(".jsonl") or (not filename.startswith("refinado_") and filename.endswith(".json")):
                os.remove(DATA_DIR / filename)
                deleted_files_count += 1
        print(f"INFO: {deleted_files_count} arquivos removidos.")
    DATA_DIR.mkdir(exist_ok=True)
    VIDEOS_DIR.mkdir(exist_ok=True)
    try:
        print("INFO: Carregando modelos de IA (Whisper e Gemini)...")
        whisper_model = whisper.load_model("base")
        api_key = os.environ.get("GEMINI_API_KEY")
        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=api_key, temperature=0.1)
        refiner_chain = REFINER_PROMPT_JSON | llm | JsonOutputParser()
        print("‚úÖ Modelos carregados.")
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO na inicializa√ß√£o dos modelos: {e}")
        return

    json_paths_to_refine = []

    # --- Etapa 1: Processar v√≠deos do YouTube ---
    # ... (c√≥digo inalterado)
    if LINKS_FILE.exists():
        video_sources = []
        with open(LINKS_FILE, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split(',')
                    if len(parts) >= 1 and parts[0].startswith('http'):
                        url = parts[0]
                        source_type = parts[1].strip() if len(parts) > 1 else 'video_tutorial'
                        video_sources.append({'url': url, 'type': source_type})
        if video_sources:
            print(f"\n--- INICIANDO PROCESSAMENTO DE {len(video_sources)} V√çDEOS DO YOUTUBE ---")
            for index, source in enumerate(video_sources):
                print(
                    f"\n--- Processando V√≠deo {index + 1}/{len(video_sources)}: {source['url']} (Tipo: {source['type']}) ---")
                json_path = transcribe_youtube_video(source['url'], whisper_model)
                if json_path:
                    json_paths_to_refine.append({'path': json_path, 'type': source['type']})

    # --- Etapa 2: Processar v√≠deos locais ---
    # ... (c√≥digo inalterado)
    local_video_files = list(VIDEOS_DIR.glob("*.mp4")) + list(VIDEOS_DIR.glob("*.m4a")) + list(VIDEOS_DIR.glob("*.mov"))
    if local_video_files:
        print(f"\n--- INICIANDO PROCESSAMENTO DE {len(local_video_files)} V√çDEOS LOCAIS ---")
        for index, video_file in enumerate(local_video_files):
            print(f"\n--- Processando V√≠deo Local {index + 1}/{len(local_video_files)}: {video_file.name} ---")
            source_type = 'video_local'
            json_path = transcribe_local_video(video_file, whisper_model)
            if json_path:
                json_paths_to_refine.append({'path': json_path, 'type': source_type})

    # --- NOVO ---: Etapa 3: Processar documentos TXT e PDF da pasta DATA
    document_files = list(DATA_DIR.glob("*.txt")) + list(DATA_DIR.glob("*.pdf"))
    if document_files:
        print(f"\n--- INICIANDO PROCESSAMENTO DE {len(document_files)} DOCUMENTOS LOCAIS ---")
        for index, doc_file in enumerate(document_files):
            print(f"\n--- Processando Documento {index + 1}/{len(document_files)}: {doc_file.name} ---")
            json_path = None
            source_type = ''
            if doc_file.suffix == '.txt':
                source_type = 'documento_texto'
                json_path = process_text_file(doc_file)
            elif doc_file.suffix == '.pdf':
                source_type = 'documento_pdf'
                json_path = process_pdf_file(doc_file)

            if json_path:
                json_paths_to_refine.append({'path': json_path, 'type': source_type})

    # --- Etapa 4: Refinar todos os JSONs coletados ---
    # ... (c√≥digo inalterado)
    if json_paths_to_refine:
        print(f"\n--- INICIANDO ETAPA DE REFINAMENTO PARA {len(json_paths_to_refine)} FONTES ---")
        for source in json_paths_to_refine:
            refine_single_json_file(source['path'], refiner_chain, source['type'])

    # --- Etapa 5: Criar o banco de dados final ---
    create_database_from_all_jsonl()

    # --- Limpeza Final ---
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)
    print("\n--- PIPELINE CONCLU√çDO COM SUCESSO! ---")


if __name__ == "__main__":
    main()