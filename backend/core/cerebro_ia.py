import os
import json
import chromadb
from typing import Dict, Any, List, Optional
from pathlib import Path
from dotenv import load_dotenv
from urllib.parse import urlparse
import traceback

# IMPORTS EST√ÅVEIS (LangChain 0.1.x)
from langchain.docstore.document import Document
from langchain.prompts import ChatPromptTemplate

try:
    from langchain.retrievers import EnsembleRetriever
except ImportError:
    from langchain.retrievers.ensemble import EnsembleRetriever

from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from pydantic import BaseModel, Field
from chromadb.config import Settings
from thefuzz import fuzz

from core.shared import IA_MODELS, print_error, print_info, print_success, print_warning

# --- CONFIGURA√á√ïES ---
CORE_DIR = Path(__file__).parent.resolve()
BACKEND_DIR = CORE_DIR.parent.resolve()
DATA_DIR = BACKEND_DIR / "data"
PLAYBOOK_PATH = str(DATA_DIR / "playbook_vendas.json")
GEMINI_MODEL_NAME = "gemini-2.5-flash"
env_path = BACKEND_DIR / ".env"
load_dotenv(dotenv_path=env_path)


# --- CLASSES DE SA√çDA ---
class AIResponse(BaseModel):
    sugestao_resposta: str = Field(description="A sugest√£o de resposta para o vendedor enviar.")
    proximo_passo: Optional[str] = Field(description="Uma sugest√£o de a√ß√£o ou pergunta futura.")


class StageTransitionDecision(BaseModel):
    proximo_stage_id: str
    justificativa: str


# --- SERVI√áO PRINCIPAL ---
class SalesCopilot:
    def __init__(self, llm, retriever, playbook, embeddings):
        self.llm = llm
        self.retriever = retriever
        self.playbook = playbook
        self.embeddings = embeddings

        self.prompt = ChatPromptTemplate.from_template("""
        Voc√™ √© o VENAI, um assistente de vendas experiente.

        CONTEXTO DA CONVERSA (√öltimas mensagens):
        {history_context}

        MANUAL T√âCNICO / CONHECIMENTO (RAG):
        {tech_context}

        PERGUNTA OU A√á√ÉO ATUAL DO CLIENTE:
        "{query}"

        OBJETIVO: Ajudar o vendedor a responder de forma persuasiva e t√©cnica.

        Responda ESTRITAMENTE neste formato JSON:
        {{
            "sugestao_resposta": "Texto da resposta...",
            "proximo_passo": "Sugest√£o do que fazer a seguir (opcional)"
        }}
        """)
        self.chain = self.prompt | self.llm.with_structured_output(AIResponse)

    # üí° CORRE√á√ÉO AQUI: Argumentos renomeados para bater com o main.py
    def generate_sales_suggestions(self, query, full_conversation_history, current_stage_id, is_private_query,
                                   client_data):
        print_info(f"ü§ñ [IA] Gerando sugest√£o para: '{query}'")

        # Usa a vari√°vel com o nome novo
        history = full_conversation_history

        # 1. Prepara o Hist√≥rico
        recent_msgs = history[-10:] if history else []
        history_text = "\n".join([f"{m.get('sender', '?').upper()}: {m.get('content', '')}" for m in recent_msgs])

        # 2. Busca Conhecimento T√©cnico (RAG)
        tech_text = "Nenhuma informa√ß√£o t√©cnica encontrada."
        if self.retriever:
            try:
                docs = self.retriever.invoke(query)
                if docs:
                    tech_text = "\n\n".join([d.page_content for d in docs])
                    print_success(f"üìö [IA] Encontrados {len(docs)} documentos t√©cnicos.")
            except Exception as e:
                print_warning(f"‚ö†Ô∏è [IA] Erro no retriever: {e}")

        # 3. Chama o LLM
        try:
            if is_private_query:
                # Prompt Espec√≠fico para Consultas Internas
                internal_prompt = ChatPromptTemplate.from_template("""
                Voc√™ √© o VENAI, um assistente s√™nior de vendas.
                
                CONTEXTO T√âCNICO (RAG):
                {tech_context}
                
                PERGUNTA DO VENDEDOR:
                "{query}"
                
                OBJETIVO: Responder a d√∫vida do vendedor de forma direta, t√©cnica e informativa.
                N√ÉO sugira uma resposta para o cliente.
                N√ÉO sugira pr√≥ximos passos.
                Apenas responda a pergunta.
                
                Responda ESTRITAMENTE neste formato JSON:
                {{
                    "sugestao_resposta": "Sua resposta informativa para o vendedor...",
                    "proximo_passo": null
                }}
                """)
                chain = internal_prompt | self.llm.with_structured_output(AIResponse)
                resp = chain.invoke({
                    "tech_context": tech_text,
                    "query": query
                })
            else:
                # Prompt Padr√£o (Sugest√£o de Resposta)
                resp = self.chain.invoke({
                    "history_context": history_text,
                    "tech_context": tech_text,
                    "query": query
                })

            return {
                "status": "success",
                "suggestions": {
                    "immediate_answer": resp.sugestao_resposta,
                    "follow_up_options": [
                        {"text": resp.proximo_passo, "is_recommended": True}] if resp.proximo_passo else []
                }
            }
        except Exception as e:
            print_error(f"‚ùå [IA] Erro ao gerar resposta: {e}")
            traceback.print_exc()
            return {"status": "error", "suggestions": {"immediate_answer": "Erro ao processar IA."}}


# --- FACTORY ---
def initialize_chroma_client():
    """
    Inicializa cliente Chroma LOCAL (PersistentClient).
    N√£o usa mais servidor HTTP remoto.
    """
    try:
        # Define o diret√≥rio de persist√™ncia
        persist_dir = str(DATA_DIR / "chroma_db")
        print_info(f"üîó [IA] Inicializando Chroma LOCAL em: {persist_dir}")
        
        client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Testa listando collections
        collections = client.list_collections()
        print_success(f"‚úÖ [IA] Chroma LOCAL inicializado! Collections: {[c.name for c in collections]}")
        return client
    except Exception as e:
        print_error(f"‚ùå [IA] Erro ao inicializar Chroma LOCAL: {e}")
        traceback.print_exc()
        return None


def load_models(client):
    if not client:
        print_warning("‚ö†Ô∏è [IA] Cliente Chroma n√£o dispon√≠vel, pulando carregamento de modelos")
        return None, None, None, None
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print_error("‚ùå [IA] GEMINI_API_KEY n√£o configurada no .env")
        return None, None, None, None

    try:
        print_info("ü§ñ [IA] Inicializando LLM Gemini...")
        llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, google_api_key=api_key, temperature=0.2)
        print_success(f"‚úÖ [IA] LLM {GEMINI_MODEL_NAME} carregado")
    except Exception as e:
        print_error(f"‚ùå [IA] Erro ao carregar LLM: {e}")
        return None, None, None, None

    try:
        print_info("üìù [IA] Inicializando Embeddings...")
        embed = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
        print_success("‚úÖ [IA] Embeddings carregados")
    except Exception as e:
        print_error(f"‚ùå [IA] Erro ao carregar Embeddings: {e}")
        return llm, None, None, None

    retriever = None
    try:
        print_info("üîç [IA] Configurando Retriever...")
        db = Chroma(client=client, collection_name="evolution", embedding_function=embed)
        try:
            # Verifica se tem dados
            count = db._collection.count()
            print_info(f"üìä [IA] Collection 'evolution' tem {count} documentos")
            
            if count == 0:
                print_warning("‚ö†Ô∏è [IA] Collection vazia, usando BM25 com documento placeholder")
                retriever = BM25Retriever.from_documents([Document(page_content="vazio")])
            else:
                # Retriever H√≠brido (BM25 + Vetor)
                all_docs = db._collection.get()
                docs_objs = [Document(page_content=t, metadata=m or {}) for t, m in
                             zip(all_docs['documents'], all_docs['metadatas'])]
                bm25 = BM25Retriever.from_documents(docs_objs)
                bm25.k = 3
                chroma_retriever = db.as_retriever(search_kwargs={"k": 3})
                retriever = EnsembleRetriever(retrievers=[bm25, chroma_retriever], weights=[0.4, 0.6])
                print_success("‚úÖ [IA] Retriever H√≠brido (BM25 + Vetor) configurado")
        except Exception as e:
            print_warning(f"‚ö†Ô∏è [IA] Erro ao criar retriever h√≠brido, usando fallback: {e}")
            retriever = db.as_retriever()
    except Exception as e:
        print_error(f"‚ùå [IA] Erro ao configurar Retriever: {e}")
        retriever = None

    return llm, retriever, embed, {}


def get_sales_copilot():
    if not IA_MODELS.get("llm"): return None
    return SalesCopilot(IA_MODELS["llm"], IA_MODELS["retriever"], IA_MODELS["playbook"], IA_MODELS["embeddings"])