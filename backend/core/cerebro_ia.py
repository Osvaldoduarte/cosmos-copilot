import os
import json
import chromadb
from typing import Dict, Any, List, Optional
from pathlib import Path
from dotenv import load_dotenv
from urllib.parse import urlparse
import traceback

from langchain.docstore.document import Document
from langchain.prompts import ChatPromptTemplate
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from pydantic import BaseModel, Field
from chromadb.config import Settings
from thefuzz import fuzz

# üí° CORRE√á√ÉO: Importa o reposit√≥rio VERDADEIRO
from repositories.chroma_repository import get_conversations_repository, ChromaConversationsRepository
from core.shared import IA_MODELS, print_error, print_info, print_success, print_warning  # Importa os globais

# --- CONFIGURA√á√ïES GLOBAIS ---
CHROMA_CLIENT = None
CORE_DIR = Path(__file__).parent.resolve()
BACKEND_DIR = CORE_DIR.parent.resolve()
DATA_DIR = BACKEND_DIR / "data"
CHROMA_PATH = str(BACKEND_DIR / "chroma_db_local")
CHROMA_CONVERSAS_PATH = str(BACKEND_DIR / "chroma_db_conversas")
PLAYBOOK_PATH = str(DATA_DIR / "playbook_vendas.json")
GEMINI_MODEL_NAME = "gemini-2.5-flash"
env_path = BACKEND_DIR / ".env"
load_dotenv(dotenv_path=env_path)
if not os.environ.get("GEMINI_API_KEY"):
    print_warning(f"ALERTA: N√£o foi poss√≠vel carregar as vari√°veis do arquivo: {env_path}")
CHROMA_HOST = os.environ.get("CHROMA_HOST")
api_key = os.environ.get("GEMINI_API_KEY")


# --- DEFINI√á√ïES DE Pydantic (permanecem as mesmas) ---
class StageTransitionDecision(BaseModel):
    proximo_stage_id: str = Field(
        description="O ID do pr√≥ximo est√°gio mais l√≥gico para o qual a conversa deve avan√ßar, escolhido estritamente a partir da lista de 'ROTAS POSS√çVEIS'.")
    justificativa: str = Field(
        description="Uma breve justificativa da sua escolha, referenciando o hist√≥rico da conversa.")


class AIResponse(BaseModel):
    sugestao_resposta: str = Field(
        description="A resposta direta e factual para a pergunta t√©cnica do cliente, baseada no CONTEXTO T√âCNICO.")
    proximo_passo: Optional[str] = Field(
        description="Uma pergunta ou sugest√£o de pr√≥ximo passo para o vendedor enviar ao cliente, alinhada com o OBJETIVO ESTRAT√âGICO.")


class ClientData(BaseModel):
    """Estrutura para armazenar os dados extra√≠dos do cliente."""
    nome: Optional[str] = Field(None, description="O nome do cliente, se mencionado.")
    empresa: Optional[str] = Field(None, description="O nome da empresa do cliente, se mencionada.")
    gerente: Optional[str] = Field(None, description="O nome do gerente ou decisor mencionado pelo cliente.")
    necessidades: Optional[List[str]] = Field(None,
                                              description="Uma lista de dores ou necessidades expl√≠citas do cliente (ex: 'emiss√£o de notas', 'valores de mensalidade').")


# --- Templates e Prompts (permanecem os mesmos) ---
# ... (SUPER_PROMPT_TEMPLATE, TRIAGE_PROMPT_TEMPLATE, STAGE_DECISION_TEMPLATE, CLIENT_DATA_EXTRACTION_TEMPLATE) ...

SUPER_PROMPT_TEMPLATE = """
Voc√™ √© o "VENAI", um assistente de vendas especialista em IA para o sistema CosmosERP.

Sua miss√£o √© analisar o contexto completo de uma intera√ß√£o com o cliente e gerar uma resposta estruturada em JSON.

---
ARQUIVO DO CLIENTE (FATOS CONHECIDOS)
Estes s√£o os dados estruturados que j√° conhecemos sobre o cliente. Use-os para personalizar sua resposta.
{client_data}
---
CONTEXTO GERAL (C√âREBRO 2 - O HIST√ìRICO RELEVANTE)
Este √© o hist√≥rico recente ou trechos relevantes da conversa. Use-o para entender o que foi dito.
{conversation_history}
---
OBJETIVO ESTRAT√âGICO (C√âREBRO 3 - O PLAYBOOK DE VENDAS)
Com base na conversa, o est√°gio atual da venda √© '{stage_name}'. O seu objetivo agora √©: '{stage_goal}'.
---
EVID√äNCIAS T√âCNICAS (C√âREBRO 1 - A BASE DE CONHECIMENTO)
Para responder √† pergunta do cliente, utilize estritamente as seguintes informa√ß√µes t√©cnicas sobre o produto. N√£o invente funcionalidades.
{technical_context}
---

PERGUNTA ATUAL: "{query}"

INSTRU√á√ÉO CR√çTICA:
- Se a "PERGUNTA ATUAL" for do cliente, seu objetivo √© respond√™-lo e avan√ßar a venda. Gere 'sugestao_resposta' e 'proximo_passo'.
- Se a "PERGUNTA ATUAL" for uma consulta interna do vendedor (ex: "quem √© Cristiano?", "qual o valor?"), seu objetivo √© responder APENAS ao vendedor. Use o "ARQUIVO DO CLIENTE" e o "CONTEXTO GERAL" para encontrar a resposta. Gere apenas 'sugestao_resposta' e retorne 'proximo_passo' como nulo.

Baseado em TUDO acima, gere sua resposta.
"""

TRIAGE_PROMPT_TEMPLATE = """
Analise a mensagem do usu√°rio e classifique-a estritamente em uma das seguintes categorias:
'saudacao_inicial', 'resposta_qualificacao', 'pergunta_tecnica', 'escolha_de_opcao', 'pergunta_conversacional', 'comentario_geral'.

Sua resposta deve ser APENAS o nome da categoria correspondente, em min√∫sculas, sem aspas, espa√ßos extras ou qualquer outra pontua√ß√£o.

Mensagem do usu√°rio: "{query}"
Categoria:
"""
TRIAGE_PROMPT = ChatPromptTemplate.from_template(TRIAGE_PROMPT_TEMPLATE)

STAGE_DECISION_TEMPLATE = """
Voc√™ √© o "Gerente de Est√°gios de Vendas", e sua √∫nica tarefa √© analisar o contexto e decidir para qual est√°gio a conversa deve avan√ßar.

---
CONTEXTO GERAL (O CLIENTE E A CONVERSA)
Este √© o hist√≥rico da conversa. Use-o para entender o ponto de partida e o que levou √† pergunta atual.
{conversation_history}
---
EST√ÅGIO ATUAL: {current_stage_id}

ROTAS POSS√çVEIS PARA O PR√ìXIMO EST√ÅGIO:
Abaixo est√° uma lista de pr√≥ximos est√°gios poss√≠veis e as condi√ß√µes para ir para cada um.
Voc√™ DEVE escolher o 'proximo_stage_id' estritamente a partir desta lista.
{possible_routes}
---

√öLTIMA A√á√ÉO / PERGUNTA: "{query}"

Decida e justifique o pr√≥ximo 'proximo_stage_id' em formato JSON.
"""
STAGE_DECISION_PROMPT = ChatPromptTemplate.from_template(STAGE_DECISION_TEMPLATE)

CLIENT_DATA_EXTRACTION_TEMPLATE = """
Sua √∫nica tarefa √© analisar um hist√≥rico de conversa e extrair as seguintes informa√ß√µes sobre o cliente: nome, empresa, nome do gerente (se houver) e uma lista de suas necessidades.
Se uma informa√ß√£o n√£o for mencionada, retorne 'null' para aquele campo.

Hist√≥rico da Conversa:
{conversation_history}
"""


# --- FUN√á√ïES AUXILIARES (Permanecem as mesmas ou s√£o movidas para m√©todos) ---

def extract_client_data_from_history(llm: ChatGoogleGenerativeAI, conversation_history: str) -> ClientData:
    """
    Usa um LLM para extrair dados estruturados (nome, empresa, etc.) do hist√≥rico de uma conversa.
    """
    print_info("üß† C√âREBRO 2.5: Extraindo dados estruturados do cliente do hist√≥rico...")
    try:
        # Monta a cadeia para extra√ß√£o com sa√≠da estruturada
        prompt = ChatPromptTemplate.from_template(CLIENT_DATA_EXTRACTION_TEMPLATE)
        chain = prompt | llm.with_structured_output(ClientData)

        # Invoca a cadeia com o hist√≥rico
        extracted_data = chain.invoke({"conversation_history": conversation_history})

        print_success(f"‚úÖ C√âREBRO 2.5: Dados extra√≠dos: {extracted_data.dict()}")
        return extracted_data
    except Exception as e:
        print_error(f"‚ùå ERRO ao extrair dados do cliente: {e}")
        traceback.print_exc()
        # Retorna um objeto vazio em caso de erro
        return ClientData()


def decide_next_stage(llm: ChatGoogleGenerativeAI, conversation_history: str, current_stage_id: str,
                      possible_routes: str, query: str) -> str:
    # (Mantenha esta fun√ß√£o auxiliar fora da classe, pois ela n√£o depende de self.retriever ou self.playbook,
    # mas sim do LLM e dos dados de entrada, sendo mais f√°cil de testar isoladamente.)
    """
    Fun√ß√£o dedicada a usar o LLM para determinar o pr√≥ximo est√°gio de vendas.
    """
    print_info(f"üîÑ C√âREBRO 3: Iniciando tomada de decis√£o...")
    try:
        # 1. Monta o prompt
        prompt = STAGE_DECISION_PROMPT

        # 2. Constr√≥i a cadeia
        chain = prompt | llm.with_structured_output(StageTransitionDecision)

        # 3. Invoca a cadeia
        decision = chain.invoke({
            "conversation_history": conversation_history,
            "current_stage_id": current_stage_id,
            "possible_routes": possible_routes,
            "query": query
        })

        print_success(f"‚úÖ C√âREBRO 3: Decis√£o tomada. Pr√≥ximo ID: {decision.proximo_stage_id}.")
        return decision.proximo_stage_id

    except Exception as e:
        print_error(f"[DEBUG C3] ERRO durante a decis√£o de est√°gio: {e}")
        traceback.print_exc()
        print_error(f"‚ùå ERRO ao decidir o pr√≥ximo est√°gio. Retornando est√°gio atual: {current_stage_id}.")
        return current_stage_id


def get_dynamic_conversation_context(
        conversation_history: List[Dict[str, Any]],
        query: str,
        embedding_function
) -> str:
    # (Esta fun√ß√£o auxiliar de RAG em mem√≥ria continua a mesma)
    """
    Usa uma BUSCA H√çBRIDA MANUAL com FUZZY MATCHING e Sem√¢ntica para criar o contexto da conversa.
    """
    if not conversation_history: return "Nenhum hist√≥rico de conversa fornecido."
    print_info("üß† C√âREBRO 2 (FUZZY H√çBRIDO v2): Criando RAG em mem√≥ria...")

    docs = [Document(page_content=msg["content"], metadata=msg) for msg in conversation_history if msg.get("content")]
    if not docs: return "Nenhum hist√≥rico de conversa encontrado."

    # --- L√ìGICA DE BUSCA H√çBRIDA MANUAL APRIMORADA ---
    keyword_hits = []
    query_words = {word.lower().strip('.,?!') for word in query.split()}
    similarity_threshold = 85

    for doc in docs:
        doc_words = [word.lower().strip('.,?!') for word in doc.page_content.split()]
        for q_word in query_words:
            if any(fuzz.ratio(q_word, d_word) > similarity_threshold for d_word in doc_words):
                keyword_hits.append(doc);
                break

    print_info(
        f"üß† C√âREBRO 2 (FUZZY H√çBRIDO v2): Encontradas {len(keyword_hits)} mensagens por palavra-chave aproximada.")

    # Busca Sem√¢ntica
    # Nota: Este √© um uso ineficiente do ChromaDB (cria e destr√≥i o DB a cada chamada),
    # mas √© necess√°rio para o RAG em mem√≥ria se voc√™ n√£o quiser usar um √≠ndice mais complexo como o FAISS.
    db_temp = Chroma.from_documents(docs, embedding_function)
    vector_retriever = db_temp.as_retriever(search_kwargs={"k": 5})
    semantic_hits = vector_retriever.invoke(query)
    print_info(
        f"üß† C√âREBRO 2 (FUZZY H√çBRIDO v2): Encontradas {len(semantic_hits)} mensagens por similaridade sem√¢ntica.")

    # Combina√ß√£o e Limpeza
    combined_docs = keyword_hits + semantic_hits
    seen_content = set();
    unique_docs = []
    for doc in combined_docs:
        content_key = doc.page_content.strip()
        if content_key not in seen_content:
            seen_content.add(content_key);
            unique_docs.append(doc)

    print_info(f"üß† C√âREBRO 2 (FUZZY H√çBRIDO v2): Contexto combinado com {len(unique_docs)} mensagens √∫nicas.")

    if not unique_docs:
        print_warning("Nenhum documento relevante encontrado. Usando as 5 √∫ltimas mensagens como fallback.")
        unique_docs = docs[-5:]

    unique_docs.sort(key=lambda doc: float(doc.metadata.get('timestamp', 0)))

    formatted_context = "\n".join(
        [f"{doc.metadata.get('sender', 'desconhecido').capitalize()}: {doc.page_content}" for doc in unique_docs])

    return formatted_context


def get_relevant_video_suggestion(ensemble_retriever: EnsembleRetriever, query: str) -> Optional[Dict[str, str]]:
    """
    Busca o documento mais relevante para a query e extrai o link do v√≠deo de seus metadados.
    (Fun√ß√£o mantida para ser chamada pela classe SalesCopilot)
    """
    print_info("üé¨ C√âREBRO 4: Buscando sugest√£o de v√≠deo...")
    try:
        context_docs = ensemble_retriever.invoke(query, k=1)

        if not context_docs:
            print_error("‚ùå C√âREBRO 4: Nenhum documento relevante encontrado para v√≠deo.")
            return None

        doc = context_docs[0]
        metadata = doc.metadata

        video_url = metadata.get("url_video")
        video_title = metadata.get("titulo_video")

        if video_url and video_title and ("youtube.com" in video_url or "youtu.be" in video_url):
            print_success(f"‚úÖ C√âREBRO 4: V√≠deo sugerido: {video_title}")
            return {
                "title": video_title,
                "url": video_url
            }
        else:
            print_info("‚ÑπÔ∏è C√âREBRO 4: O documento mais relevante n√£o possui metadados de v√≠deo ou n√£o √© um v√≠deo.")
            return None

    except Exception as e:
        print_error(f"‚ùå ERRO ao buscar sugest√£o de v√≠deo: {e}")
        return None


# --- CLASSE DE SERVI√áO (DIP/SOLID) ---
class SalesCopilot:
    """
    Servi√ßo principal para orquestrar a l√≥gica de IA, incluindo RAG e chamada ao LLM.
    """

    def __init__(
            self,
            llm: ChatGoogleGenerativeAI,
            retriever: EnsembleRetriever,
            playbook: Dict[str, Any],
            embeddings_model: GoogleGenerativeAIEmbeddings  # Adicionado para uso no RAG de mem√≥ria
    ):
        """
        Docstring (Google Style):
        Inicializa o Copilot com as depend√™ncias da LLM, RAG, Playbook e Embeddings.
        """
        self.llm = llm
        self.retriever = retriever
        self.playbook = playbook
        self.embeddings_model = embeddings_model

        # Cria a Chain de LLM e o Prompt principal uma vez na inicializa√ß√£o (Singleton Pattern para o Prompt)
        self.super_prompt = ChatPromptTemplate.from_template(SUPER_PROMPT_TEMPLATE)
        self.main_chain = self.super_prompt | self.llm.with_structured_output(AIResponse)

    def _get_technical_context(self, query: str) -> str:
        """Busca o contexto t√©cnico via RAG (C√©rebro 1)."""
        print_info("üìö C√âREBRO 1: Iniciando busca de contexto t√©cnico (RAG)...")
        context_docs = self.retriever.invoke(query)

        technical_context = "\n\n".join(
            [doc.page_content for doc in context_docs]) or "Nenhum contexto t√©cnico relevante encontrado."

        print_success("üìö C√âREBRO 1: Contexto t√©cnico recuperado.")
        return technical_context

    # ‚úÖ M√âTODO PRINCIPAL REFATORADO
    def generate_sales_suggestions(
            self,
            query: str,
            full_conversation_history: List[Dict[str, Any]],
            current_stage_id: str,
            is_private_query: bool,
            client_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Docstring (Google Style): Gera a sugest√£o de resposta e o pr√≥ximo passo para o vendedor.

        Args:
            query: Mensagem mais recente.
            full_conversation_history: Hist√≥rico de mensagens completo.
            current_stage_id: ID do est√°gio atual.
            is_private_query: Se a query √© interna do vendedor.
            client_data: Dados estruturados extra√≠dos do cliente.

        Returns:
            Um dicion√°rio contendo o payload de sugest√£o e o pr√≥ximo ID de est√°gio.
        """
        print_info("\n--- INICIANDO FLUXO DE GERA√á√ÉO ESTRAT√âGICO V6.0 (CLASSE DE SERVI√áO) ---")

        # ETAPA 1: C√©rebro 2 (Hist√≥rico H√≠brido)
        conversation_context = get_dynamic_conversation_context(
            full_conversation_history,
            query,
            self.embeddings_model
        )
        print_info(f"üß† C√âREBRO 2: Contexto h√≠brido da conversa carregado.")

        # ETAPA 2: Decis√£o de Est√°gio (C√©rebro 3)
        if is_private_query:
            print_warning("‚ö°Ô∏è ROTA R√ÅPIDA: Consulta privada do vendedor. Pulando C√©rebro 3.")
            stage_name = "Consulta Interna"
            stage_goal = "Responder a uma pergunta do vendedor com base no hist√≥rico."
            final_next_stage_id = current_stage_id
        else:
            print_info("üåê ROTA COMPLETA: An√°lise de mensagem do cliente. Executando C√©rebro 3.")
            if not current_stage_id: current_stage_id = self.playbook["initial_stage"]
            current_stage_info = self.playbook["stages"].get(current_stage_id, {})
            stage_name = current_stage_info.get("name", "An√°lise Inicial")
            stage_goal = current_stage_info.get("goal", "Responder √† d√∫vida e avan√ßar a conversa.")
            possible_routes = "\n".join(
                [f"- stage_id: {stage['stage_id']}, condition: {stage['condition']}" for stage in
                 current_stage_info.get("possible_next_stages",
                                        [])]) or "Nenhuma rota de pr√≥ximo est√°gio definida."
            try:
                final_next_stage_id = decide_next_stage(
                    llm=self.llm, conversation_history=conversation_context,
                    current_stage_id=current_stage_id, possible_routes=possible_routes, query=query
                )
            except Exception as e:
                print_error(f"FALHA NO C√âREBRO 3 (Decis√£o de Est√°gio): {e}. Mantendo o est√°gio atual como fallback.")
                final_next_stage_id = current_stage_id

        # ETAPA 3: C√©rebro 1 (RAG T√©cnico)
        technical_context = self._get_technical_context(query)

        # ETAPA 4: S√≠ntese e Chamada ao LLM (Super Prompt)
        client_data_text = json.dumps(client_data, indent=2,
                                      ensure_ascii=False) if client_data else "Nenhum dado estruturado sobre o cliente foi coletado ainda."
        print_info("üöÄ Montando Super Prompt e fazendo a chamada √∫nica ao LLM...")

        # Usa a chain pr√©-constru√≠da
        ai_response = self.main_chain.invoke({
            "client_data": client_data_text, "conversation_history": conversation_context,
            "stage_name": stage_name, "stage_goal": stage_goal,
            "technical_context": technical_context, "query": query
        })
        print_success("‚úÖ LLM retornou uma resposta estruturada.")

        # ETAPAS FINAIS (V√≠deo e Formata√ß√£o do Payload)
        video_suggestion = get_relevant_video_suggestion(self.retriever, query)

        suggestion_payload = {"immediate_answer": ai_response.sugestao_resposta, "follow_up_options": []}

        if ai_response.proximo_passo:
            suggestion_payload["follow_up_options"].append({"text": ai_response.proximo_passo, "is_recommended": True})

        if video_suggestion:
            suggestion_payload["video"] = video_suggestion

        return {"status": "success", "new_stage_id": final_next_stage_id, "suggestions": suggestion_payload}


# --- Fun√ß√µes de Inicializa√ß√£o e DI (Singleton/Factory) ---

def initialize_chroma_client():
    # ... (Mantenha a fun√ß√£o initialize_chroma_client existente) ...
    """Inicializa e armazena o cliente ChromaDB HttpClient para v1.2.2."""
    global CHROMA_CLIENT
    CHROMA_SERVER_URL = os.environ.get("CHROMA_HOST")

    if CHROMA_CLIENT is None and CHROMA_SERVER_URL:
        print_info(f"Conectando ao ChromaDB v1.2.2 em {CHROMA_SERVER_URL}")
        try:
            if not CHROMA_SERVER_URL.startswith(('http://', 'https://')):
                CHROMA_SERVER_URL = 'https://' + CHROMA_SERVER_URL

            parsed_url = urlparse(CHROMA_SERVER_URL)
            host = parsed_url.netloc.split(':')[0] if parsed_url.netloc else parsed_url.path.split(':')[0]
            ssl_enabled = parsed_url.scheme == 'https'
            port = parsed_url.port or (443 if ssl_enabled else 80)

            if not host:
                raise ValueError("N√£o foi poss√≠vel extrair o hostname da CHROMA_HOST URL.")

            print_info(f"Usando HttpClient com host='{host}', port={port}, ssl={ssl_enabled}")

            CHROMA_CLIENT = chromadb.HttpClient(
                host=host,
                ssl=ssl_enabled,
                port=port
            )
            print_info("Testando conex√£o com heartbeat...")
            CHROMA_CLIENT.heartbeat()
            print_success("ChromaDB Cliente v1.2.2 conectado com sucesso!")

        except Exception as e:
            print_error(f"Falha na conex√£o ChromaDB v1.2.2: {e}")
            traceback.print_exc()
            return None
    return CHROMA_CLIENT


def load_models(chroma_client_instance) -> tuple:
    # ... (Mantenha a fun√ß√£o load_models existente, ela carrega as depend√™ncias no IA_MODELS) ...
    """Carrega modelos e inicializa retrievers para v1.2.2 (Substitui AMBAS as vers√µes antigas)"""
    load_dotenv()
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key: raise ValueError("ERRO: Chave GEMINI_API_KEY n√£o configurada.")

    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_NAME,
        google_api_key=api_key,
        temperature=0.1
    )

    embeddings_model_langchain = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004",
        google_api_key=api_key
    )

    COLLECTION_NAME = "evolution"

    print_info(f"Conectando LangChain Chroma √† collection '{COLLECTION_NAME}'...")
    try:
        db_tecnico = Chroma(
            client=chroma_client_instance,
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings_model_langchain
        )

        native_collection = db_tecnico._collection

        count = native_collection.count()
        print_success(f"Conectado √† collection '{COLLECTION_NAME}'. Documentos encontrados: {count}")

        if count == 0:
            raise FileNotFoundError(
                "ERRO: Banco de Dados T√©cnico (ChromaDB) est√° vazio. Execute o pipeline de ingest√£o.")

    except Exception as e:
        print_error(f"Erro ao conectar LangChain Chroma √† collection: {e}")
        traceback.print_exc()
        raise

    print_info("Preparando retrievers para Busca H√≠brida...")
    all_docs_resp = native_collection.get(include=["metadatas", "documents"])

    if not all_docs_resp or not all_docs_resp.get('documents'):
        raise FileNotFoundError("ERRO: Falha ao buscar documentos da cole√ß√£o remota.")

    docs_list = [
        Document(page_content=doc, metadata=meta or {})
        for doc, meta in zip(all_docs_resp['documents'], all_docs_resp['metadatas'])
    ]
    print(f"INFO: {len(docs_list)} documentos baixados para o BM25.")

    keyword_retriever = BM25Retriever.from_documents(docs_list)
    keyword_retriever.k = 3

    vector_retriever = db_tecnico.as_retriever(search_kwargs={"k": 3})

    ensemble_retriever = EnsembleRetriever(
        retrievers=[keyword_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    print_success("Retriever H√≠brido criado")

    if not Path(PLAYBOOK_PATH).exists():
        raise FileNotFoundError(f"Playbook n√£o encontrado em {PLAYBOOK_PATH}")
    with open(PLAYBOOK_PATH, 'r', encoding='utf-8') as f:
        playbook = json.load(f)

    print_success("LLM, Embedding, DB T√©cnico e Playbook carregados")
    return llm, ensemble_retriever, embeddings_model_langchain, playbook


# --- FUN√á√ÉO DE F√ÅBRICA (FACTORY/DI) ---
def get_sales_copilot() -> SalesCopilot:
    """
    Fun√ß√£o de Inje√ß√£o de Depend√™ncia que atua como Singleton/Factory.
    Retorna uma inst√¢ncia de SalesCopilot usando os modelos globais.
    """
    # Verifica se os modelos globais foram carregados pelo main.py
    if IA_MODELS["llm"] is None or IA_MODELS["retriever"] is None or IA_MODELS["playbook"] is None or IA_MODELS[
        "embeddings"] is None:
        raise RuntimeError("Modelos de IA n√£o inicializados. Verifique se o main.py chamou init_models().")

    # Retorna a inst√¢ncia da classe de servi√ßo (o estado do servi√ßo est√° contido nela)
    return SalesCopilot(
        llm=IA_MODELS["llm"],
        retriever=IA_MODELS["retriever"],
        playbook=IA_MODELS["playbook"],
        embeddings_model=IA_MODELS["embeddings"]
    )