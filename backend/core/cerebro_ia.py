import os
import json
import chromadb
from typing import Dict, Any, List, Optional
from pathlib import Path
from dotenv import load_dotenv

from langchain.docstore.document import Document
from langchain.prompts import ChatPromptTemplate
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.pydantic_v1 import BaseModel, Field


# --- CONFIGURA√á√ïES GLOBAIS ---
CORE_DIR = Path(__file__).parent.resolve()
BACKEND_DIR = CORE_DIR.parent.resolve()
DATA_DIR = BACKEND_DIR / "data"
CHROMA_PATH = str(BACKEND_DIR / "chroma_db_local")
CHROMA_CONVERSAS_PATH = str(BACKEND_DIR / "chroma_db_conversas")
PLAYBOOK_PATH = str(DATA_DIR / "playbook_vendas.json")
GEMINI_MODEL_NAME = "gemini-2.5-flash"

# Carrega as vari√°veis de ambiente (do arquivo .env.local)
load_dotenv()

# --- NOVA L√ìGICA DE CONEX√ÉO AO CHROMA DB ---
CHROMA_HOST = os.environ.get("CHROMA_HOST")
api_key = os.environ.get("GEMINI_API_KEY")

if not api_key:
    raise ValueError("A vari√°vel de ambiente GEMINI_API_KEY n√£o foi definida.")

# Inicializa o modelo de embeddings que ser√° usado em ambos os casos
embeddings_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)

if CHROMA_HOST:
    print("‚úÖ Conectando ao banco de dados ChromaDB remoto no Cloud Run...")
    # Se a vari√°vel CHROMA_HOST existe, conecta-se ao servidor na nuvem
    # O port 443 e ssl=True s√£o para conex√µes HTTPS seguras
    chroma_client = chromadb.HttpClient(host=CHROMA_HOST, port=443, ssl=True)
else:
    print("‚ÑπÔ∏è  Usando banco de dados ChromaDB local. (Para deploy, configure CHROMA_HOST)")
    # Se n√£o, continua usando o banco de dados da pasta local
    CHROMA_PATH = str(Path(__file__).parent.parent / "chroma_db_local")
    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)

# --- DEFINI√á√ÉO DA ESTRUTURA DE SA√çDA ---
class StageTransitionDecision(BaseModel):
    proximo_stage_id: str = Field(description="O ID do pr√≥ximo est√°gio mais l√≥gico para o qual a conversa deve avan√ßar, escolhido estritamente a partir da lista de 'ROTAS POSS√çVEIS'.")
    justificativa: str = Field(description="Uma breve justificativa da sua escolha, referenciando o hist√≥rico da conversa.")

class AIResponse(BaseModel):
    sugestao_resposta: str = Field(
        description="A resposta direta e factual para a pergunta t√©cnica do cliente, baseada no CONTEXTO T√âCNICO.")
    proximo_passo: Optional[str] = Field(
        description="Uma pergunta ou sugest√£o de pr√≥ximo passo para o vendedor enviar ao cliente, alinhada com o OBJETIVO ESTRAT√âGICO.")


# --- TEMPLATE DO NOVO SUPER PROMPT ---
SUPER_PROMPT_TEMPLATE = """
Voc√™ √© o "Cosmos Copilot", um assistente de vendas especialista em IA para o sistema CosmosERP.

Sua miss√£o √© analisar o contexto completo de uma intera√ß√£o com o cliente e gerar uma resposta estruturada em JSON contendo duas partes: uma resposta t√©cnica para a d√∫vida atual, e uma sugest√£o estrat√©gica de pr√≥ximo passo. O ID do pr√≥ximo est√°gio √© decidido externamente.

---
CONTEXTO GERAL (C√âREBRO 2 - O CLIENTE E A CONVERSA)
Este √© o hist√≥rico completo da conversa at√© agora. Use-o para entender quem √© o cliente, o que j√° foi dito e o tom da conversa.
{conversation_history}
---
OBJETIVO ESTRAT√âGICO (C√âREBRO 3 - O PLAYBOOK DE VENDAS)
Com base na conversa, o est√°gio atual da venda √© '{stage_name}'. O seu objetivo agora √©: '{stage_goal}'.

EVID√äNCIAS T√âCNICAS (C√âREBRO 1 - A BASE DE CONHECIMENTO)
Para responder √† pergunta do cliente, utilize estritamente as seguintes informa√ß√µes t√©cnicas sobre o produto. N√£o invente funcionalidades.
{technical_context}
---

PERGUNTA ATUAL DO CLIENTE: "{query}"

Baseado em TODOS os contextos acima, gere a sua resposta.
- Se a pergunta for claramente uma consulta interna do vendedor para tirar uma d√∫vida, foque em fornecer a 'sugestao_resposta' e retorne o 'proximo_passo' como nulo.
- Se a pergunta for do cliente, forne√ßa tanto a 'sugestao_resposta' quanto o 'proximo_passo'.
"""

# --- PROMPTS AUXILIARES ---
TRIAGE_PROMPT_TEMPLATE = """
Analise a mensagem do usu√°rio e classifique-a estritamente em uma das seguintes categorias:
'saudacao_inicial', 'resposta_qualificacao', 'pergunta_tecnica', 'escolha_de_opcao', 'pergunta_conversacional', 'comentario_geral'.

Sua resposta deve ser APENAS o nome da categoria correspondente, em min√∫sculas, sem aspas, espa√ßos extras ou qualquer outra pontua√ß√£o.

Mensagem do usu√°rio: "{query}"
Categoria:
"""
TRIAGE_PROMPT = ChatPromptTemplate.from_template(TRIAGE_PROMPT_TEMPLATE)

STAGE_DECISION_TEMPLATE = """
Voc√™ √© o "Gerente de Est√°gios de Vendas", e sua √∫nica tarefa √© analisar o contexto e decidir para qual est√°gio a conversa deve avan√ßar.

---
CONTEXTO GERAL (O CLIENTE E A CONVERSA)
Este √© o hist√≥rico da conversa. Use-o para entender o ponto de partida e o que levou √† pergunta atual.
{conversation_history}
---
EST√ÅGIO ATUAL: {current_stage_id}

ROTAS POSS√çVEIS PARA O PR√ìXIMO EST√ÅGIO:
Abaixo est√° uma lista de pr√≥ximos est√°gios poss√≠veis e as condi√ß√µes para ir para cada um.
Voc√™ DEVE escolher o 'proximo_stage_id' estritamente a partir desta lista.
{possible_routes}
---

√öLTIMA A√á√ÉO / PERGUNTA: "{query}"

Decida e justifique o pr√≥ximo 'proximo_stage_id' em formato JSON.
"""

STAGE_DECISION_PROMPT = ChatPromptTemplate.from_template(STAGE_DECISION_TEMPLATE)

# --- FUN√á√ïES AUXILIARES ---

def get_or_create_conversation_db(conversation_id: str, embedding_function) -> Chroma:
    persist_directory = os.path.join(CHROMA_CONVERSAS_PATH, f"convo_{conversation_id}")
    return Chroma(persist_directory=persist_directory, embedding_function=embedding_function)


def add_message_to_conversation_rag(db: Chroma, conversation_id: str, message_data: Dict[str, Any]):
    content = message_data.get("content", "")
    if not content: return
    metadata = {"conversation_id": conversation_id, "sender": message_data.get("sender"),
                "timestamp": str(message_data.get("timestamp")), }
    doc_id = message_data.get("message_id")
    db.add_documents([Document(page_content=content, metadata=metadata)], ids=[doc_id] if doc_id else None)
    print(f"INFO: Mensagem adicionada ao RAG da conversa '{conversation_id}'.")


def get_intent_from_query(llm: ChatGoogleGenerativeAI, query: str, prompt_template) -> str:
    """
    Classifica a inten√ß√£o da mensagem do cliente usando o LLM.
    """
    try:
        # A cadeia agora retorna a string limpa devido √†s instru√ß√µes do prompt
        chain = prompt_template | llm | StrOutputParser()
        intent = chain.invoke({"query": query})

        # Faz uma limpeza final e padroniza√ß√£o por seguran√ßa, embora o prompt instrua a IA a ser estrita
        return intent.strip().lower().replace("'", "").replace('"', '')
    except Exception as e:
        print(f"‚ùå ERRO ao classificar inten√ß√£o: {e}")
        # Retorna a inten√ß√£o mais segura em caso de falha de IA.
        return "comentario_geral"  # Ou "pergunta_conversacional"


def decide_next_stage(llm: ChatGoogleGenerativeAI,conversation_history: str,current_stage_id: str,possible_routes: str,query: str) -> str:
    """
    Fun√ß√£o dedicada a usar o LLM para determinar o pr√≥ximo est√°gio de vendas.
    Retorna o ID do pr√≥ximo est√°gio ou o est√°gio atual em caso de falha.
    """
    print(f"üîÑ C√âREBRO 3: Iniciando tomada de decis√£o de est√°gio...")
    try:
        # 1. Monta o prompt espec√≠fico para a decis√£o.
        prompt = STAGE_DECISION_PROMPT

        # 2. Constr√≥i a cadeia, for√ßando a sa√≠da para o StageTransitionDecision.
        # Usa o with_structured_output com o modelo de decis√£o
        chain = prompt | llm.with_structured_output(StageTransitionDecision)

        # 3. Invoca a cadeia.
        decision = chain.invoke({
            "conversation_history": conversation_history,
            "current_stage_id": current_stage_id,
            "possible_routes": possible_routes,
            "query": query
        })

        print(f"‚úÖ C√âREBRO 3: Decis√£o tomada. Pr√≥ximo ID: {decision.proximo_stage_id}. Justificativa: {decision.justificativa[:50]}...")
        # Retorna o ID do pr√≥ximo est√°gio.
        return decision.proximo_stage_id

    except Exception as e:
        print(f"‚ùå ERRO ao decidir o pr√≥ximo est√°gio. Retornando est√°gio atual: {current_stage_id}. ERRO: {e}")
        # Em caso de falha, retorna o est√°gio atual para seguran√ßa.
        return current_stage_id

def get_relevant_video_suggestion(ensemble_retriever: EnsembleRetriever, query: str) -> Optional[Dict[str, str]]:
    """
    Busca o documento mais relevante para a query e extrai o link do v√≠deo de seus metadados.
    """
    print("üé¨ C√âREBRO 4: Buscando sugest√£o de v√≠deo...")
    try:
        # Usa o retriever h√≠brido, mas limita a busca a apenas 1 documento (k=1)
        context_docs = ensemble_retriever.invoke(query, k=1)

        if not context_docs:
            print("‚ùå C√âREBRO 4: Nenhum documento relevante encontrado para v√≠deo.")
            return None

        # O documento mais relevante √© o primeiro da lista
        doc = context_docs[0]
        metadata = doc.metadata

        # O sistema de ingest√£o de dados deve salvar 'url_video' e 'titulo_video' nos metadados.
        video_url = metadata.get("url_video")
        video_title = metadata.get("titulo_video")

        # Se houver metadados de v√≠deo e o URL for de um v√≠deo (ex: YouTube), retorna a sugest√£o.
        if video_url and video_title and ("youtube.com" in video_url or "youtu.be" in video_url):
            print(f"‚úÖ C√âREBRO 4: V√≠deo sugerido: {video_title}")
            return {
                "title": video_title,
                "url": video_url
            }
        else:
            print("‚ÑπÔ∏è C√âREBRO 4: O documento mais relevante n√£o possui metadados de v√≠deo ou n√£o √© um v√≠deo.")
            return None

    except Exception as e:
        print(f"‚ùå ERRO ao buscar sugest√£o de v√≠deo: {e}")
        return None

def get_hybrid_context_history(conversation_id: str, query: str, embeddings_model, k: int = 10) -> str:
    """
    Busca um contexto h√≠brido: as 'k' mensagens mais recentes + as 'k' mais relevantes para a query.
    Isso otimiza o n√∫mero de tokens enviados para o LLM.
    """
    try:
        db = get_or_create_conversation_db(conversation_id, embeddings_model)

        # --- PARTE 1: BUSCAR MENSAGENS RELEVANTES (RAG) ---
        # Busca no banco por mensagens semanticamente similares √† pergunta atual.
        relevant_docs = db.similarity_search(query, k=k)
        print(f"üß† C√âREBRO 2: Encontradas {len(relevant_docs)} mensagens relevantes.")

        # --- PARTE 2: BUSCAR MENSAGENS RECENTES (CRONOL√ìGICO) ---
        # Pega todas as mensagens para encontrar as mais recentes.
        all_results = db.get(include=["metadatas", "documents"])
        if not all_results or not all_results.get('ids'):
            recent_docs = []
        else:
            # Monta a lista completa de mensagens.
            all_messages = [{**meta, 'content': doc} for meta, doc in
                            zip(all_results['metadatas'], all_results['documents'])]
            # Ordena pela data/hora para garantir a ordem cronol√≥gica.
            all_messages.sort(
                key=lambda x: float(x['timestamp']) if x.get('timestamp') and x['timestamp'] != 'None' else 0,
                reverse=True)
            # Pega as 'k' mensagens mais recentes (as primeiras da lista invertida).
            recent_docs_as_dict = all_messages[:k]
            # Converte de volta para o formato de Documento do LangChain.
            recent_docs = [Document(page_content=msg['content'], metadata=msg) for msg in recent_docs_as_dict]

        print(f"üß† C√âREBRO 2: Encontradas {len(recent_docs)} mensagens recentes.")

        # --- PARTE 3: COMBINAR E FORMATAR ---
        # 1. Combina as duas listas de documentos.
        combined_docs = relevant_docs + recent_docs

        # 2. Remove duplicatas, usando o conte√∫do da mensagem como chave.
        seen_content = set()
        unique_docs = []
        for doc in combined_docs:
            # A normaliza√ß√£o (strip) ajuda na identifica√ß√£o de conte√∫dos id√™nticos.
            content_key = doc.page_content.strip()
            # Adiciona apenas se for a primeira vez que esse conte√∫do √© visto
            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_docs.append(doc)

        print(f"üß† C√âREBRO 2: Contexto final com {len(unique_docs)} mensagens √∫nicas.")

        # 3. Ordena a lista final pela data/hora (timestamp), do mais antigo para o mais novo (Cronol√≥gico).
        unique_docs.sort(
            key=lambda doc: float(doc.metadata['timestamp']) if doc.metadata.get('timestamp') and doc.metadata[
                'timestamp'] != 'None' else 0,
            reverse=False  # Garante ordem crescente por tempo (mais antigo primeiro)
        )

        # 4. Formata o hist√≥rico final em uma string leg√≠vel.
        formatted_history = "\n".join(
            [f"{doc.metadata.get('sender', 'desconhecido').capitalize()}: {doc.page_content}" for doc in unique_docs]
        )

        return formatted_history if formatted_history else "Nenhum hist√≥rico de conversa encontrado."

    except Exception as e:
        print(f"‚ùå ERRO ao buscar o hist√≥rico h√≠brido da conversa: {e}")
        return "N√£o foi poss√≠vel recuperar o hist√≥rico da conversa."

# --- FUN√á√ïES PRINCIPAIS ---

def load_models() -> tuple:
    load_dotenv()
    api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: raise ValueError("ERRO: Chave GEMINI_API_KEY n√£o configurada.")

    llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, google_api_key=api_key, temperature=0.1)
    embeddings_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)

    if not Path(CHROMA_PATH).exists(): raise FileNotFoundError(f"ERRO: DB T√©cnico n√£o encontrado em {CHROMA_PATH}.")
    db_tecnico = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings_model)

    print("INFO: Preparando retrievers para a Busca H√≠brida...")
    # 1. Pega todos os documentos do nosso banco de dados t√©cnico para a busca por palavra-chave.
    all_docs = db_tecnico.get(include=["metadatas", "documents"])
    docs_list = [Document(page_content=doc, metadata=meta) for doc, meta in
                 zip(all_docs['documents'], all_docs['metadatas'])]
    if not docs_list:
        raise FileNotFoundError(
            "ERRO CR√çTICO: O Banco de Dados T√©cnico (Chroma DB) est√° vazio. Por favor, execute o script de ingest√£o (ex: create_db.py) para popular o banco antes de iniciar o servidor."
        )

    # 2. Inicializa o retriever de palavra-chave (BM25) com esses documentos.
    keyword_retriever = BM25Retriever.from_documents(docs_list)
    keyword_retriever.k = 3  # Define que ele deve retornar os 3 melhores resultados.

    # 3. Cria o retriever vetorial a partir do nosso banco ChromaDB.
    vector_retriever = db_tecnico.as_retriever(search_kwargs={"k": 3})

    # 4. Inicializa o EnsembleRetriever, combinando os dois buscadores.
    # Damos pesos iguais para a busca vetorial e a de palavra-chave.
    ensemble_retriever = EnsembleRetriever(
        retrievers=[keyword_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )
    print("‚úÖ Retriever H√≠brido (Ensemble) criado com sucesso.")

    if not Path(PLAYBOOK_PATH).exists(): raise FileNotFoundError(
        f"ERRO: Playbook de vendas n√£o encontrado em {PLAYBOOK_PATH}.")

    with open(PLAYBOOK_PATH, 'r', encoding='utf-8') as f:
        playbook = json.load(f)
    print("‚úÖ LLM, Embedding, DB T√©cnico e Playbook carregados com sucesso.")
    return llm, ensemble_retriever, embeddings_model, playbook

def generate_sales_suggestions(
        llm: ChatGoogleGenerativeAI, ensemble_retriever: EnsembleRetriever, embeddings_model: GoogleGenerativeAIEmbeddings,
        playbook: Dict[str, Any], query: str, conversation_id: str, current_stage_id: str
) -> Dict[str, Any]:
    print("\n--- INICIANDO FLUXO DE GERA√á√ÉO ESTRAT√âGICO V2.2 ---")

    # --- ETAPA 1: COLETAR CONTEXTO DO C√âREBRO 2 (HIST√ìRICO) ---
    # Usamos nossa fun√ß√£o para obter o hist√≥rico completo e cronol√≥gico da conversa.
    conversation_history = get_hybrid_context_history(conversation_id, query, embeddings_model)
    print(f"üß† C√âREBRO 2: Hist√≥rico da conversa carregado.")

    # --- ETAPA 2: DEFINIR ESTRAT√âGIA COM C√âREBRO 3 (PLAYBOOK) ---
    triage_intent = get_intent_from_query(llm, query, TRIAGE_PROMPT)

    if not current_stage_id:
        current_stage_id = playbook["initial_stage"]

    current_stage_info = playbook["stages"].get(current_stage_id, {})
    stage_name = current_stage_info.get("name", "An√°lise Inicial")

    if triage_intent == "pergunta_tecnica":
        stage_goal = current_stage_info.get("goal", "Responder a uma d√∫vida t√©cnica espec√≠fica sobre o produto.")
    elif triage_intent == "resposta_qualificacao":
        stage_goal = "Processar as informa√ß√µes fornecidas pelo cliente e confirmar o entendimento."
    else:
        stage_goal = "Manter a conversa fluindo e guiar para o pr√≥ximo passo l√≥gico."

    # --- NOVO: Extrair e formatar as rotas poss√≠veis para a DECIS√ÉO DE EST√ÅGIO ---
    possible_next_stages = current_stage_info.get("possible_next_stages", [])

    # Formatamos essa lista em uma string leg√≠vel para ser usada na fun√ß√£o decide_next_stage.
    possible_routes = "\n".join(
        [f"- stage_id: {stage['stage_id']}, condition: {stage['condition']}" for stage in possible_next_stages]
    )
    if not possible_routes:
        possible_routes = "Nenhuma rota de pr√≥ximo est√°gio definida. Mantenha o est√°gio atual."

    # --- NOVO: Tomada de decis√£o de est√°gio (Cadeia Separada) ---
    final_next_stage_id = decide_next_stage(
        llm=llm,
        conversation_history=conversation_history,
        current_stage_id=current_stage_id,
        possible_routes=possible_routes,
        query=query
    )

    # --- ETAPA 3: COLETAR EVID√äNCIAS DO C√âREBRO 1 (T√âCNICO) ---
    # Buscamos na base de conhecimento t√©cnica por informa√ß√µes relevantes para a pergunta do cliente.
    context_docs = ensemble_retriever.invoke(query)
    technical_context = "\n\n".join([doc.page_content for doc in context_docs])
    if not technical_context:
        technical_context = "Nenhum contexto t√©cnico relevante encontrado."
    print(f"üìö C√âREBRO 1: Contexto t√©cnico recuperado.")

    # --- ETAPA 4: S√çNTESE E CHAMADA √öNICA AO LLM COM O "SUPER PROMPT" ---
    # Criamos o prompt a partir do nosso template atualizado.
    prompt = ChatPromptTemplate.from_template(SUPER_PROMPT_TEMPLATE)

    # Constru√≠mos a cadeia (chain) LangChain, for√ßando a sa√≠da para o nosso modelo AIResponse.
    chain = prompt | llm.with_structured_output(AIResponse)

    print("üöÄ Montando Super Prompt e fazendo a chamada √∫nica ao LLM...")
    # Invocamos a cadeia com todos os contextos que coletamos, incluindo as novas 'rotas'.
    ai_response = chain.invoke({
        "conversation_history": conversation_history,
        "stage_name": stage_name,
        "stage_goal": stage_goal,
        "technical_context": technical_context,
        "query": query
    })

    print(f"‚úÖ LLM retornou uma resposta estruturada. Pr√≥ximo est√°gio decidido: '{ai_response.proximo_stage_id}'")

    # --- NOVO: ETAPA 5 (C√âREBRO 4) - BUSCAR SUGEST√ÉO DE V√çDEO ---
    # Chamamos a nova fun√ß√£o que busca o v√≠deo mais relevante.
    video_suggestion = get_relevant_video_suggestion(ensemble_retriever, query)

    # --- ETAPA 6 (Antiga ETAPA 5) - FORMATAR PAYLOAD PARA O FRONTEND ---
    # Mapeamos a resposta estruturada da IA para o formato que o frontend espera.
    suggestion_payload = {
        "immediate_answer": ai_response.sugestao_resposta,
        "text_options": [],
        "follow_up_options": [
            {
                "tone": "amigavel",
                "text": ai_response.proximo_passo,
                "is_recommended": True
            }
        ],
        "video": video_suggestion  # <--- INCLUS√ÉO DO OBJETO DE SUGEST√ÉO DE V√çDEO
    }

    # O novo ID de est√°gio agora vem da decis√£o externa.
    return {"status": "success", "new_stage_id": final_next_stage_id, "suggestions": suggestion_payload}
